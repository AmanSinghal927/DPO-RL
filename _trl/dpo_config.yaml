model: "gpt2-large"
checkpoint: "/scratch/as14661/dpo_base/models/gpt_2/stf_model_weights.bin"

train_dataset: "/scratch/as14661/dpo_base/data_generation/data/25k_train_{'batch_size': 512, 'max_new_tokens': 100, 'top_k': 50, 'top_p': 0.95, 'DP': False, 'deep_speed': True, 'do_sample': True, 'temperature': 0.3, 'num_beams': 'None', 'early_stopping': 'None', 'no_repeat_ngram_size': 'None'}_rewards"
train_sample: False
per_device_train_batch_size: 8
gradient_accumulation_steps: 8
# keep effective batch_size = per_device_train_batch_size*gradient_accumulation_steps to 64
learning_rate: 1e-06
logging_steps: 50
evaluation_strategy: "steps"
num_train_epochs: 1
output_dir: "dpo_descriptiveness"
report_to: "wandb"
max_target_length: 100

beta: 0.01

# generations during evaluation
eval_generation_batch_size: 512
eval_generation_max_new_tokens: 100
eval_generation_top_k: 50
eval_generation_top_p: 0.95
save_path: "/scratch/as14661/dpo_base/_trl/checkpoints"
